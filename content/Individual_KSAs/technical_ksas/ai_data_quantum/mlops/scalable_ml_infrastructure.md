---
ksa_id: scalable_ml_infrastructure
label: Scalable ML Infrastructure
category: Technical
sector: ai_data_quantum
horizon: emerging
cluster_tags:
  - "MLOps"
  - "Cloud"
  - "Distributed Computing"
description: >
  Designs distributed compute/storage environments (on-premise, cloud, GPU clusters) for training and serving large-scale ML models; balances performance, cost, and reliability.
source_frameworks:
  - "NVIDIA DGX Best Practices Guide"
  - "onet:15-1242.00"
proficiency_levels:
  - level: Awareness
    indicator: Describes why GPUs/accelerators speed up ML; runs notebook training on a single GPU instance.
  - level: Basic
    indicator: Containersizes models and deploys to managed serving services.
  - level: Intermediate
    indicator: Implements distributed training jobs, auto-scaling, and spot-capacity optimization.
  - level: Advanced
    indicator: Tunes heterogeneous hardware clusters for throughput/cost and establishes caching strategies.
  - level: Expert
    indicator: Leads organization-wide platform strategy for large-scale AI workloads; designs multicloud, multi-tenant ML platforms with dynamic resource orchestration and green-AI targets.
---